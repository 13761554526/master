{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from time import sleep\n",
    "import sys\n",
    "import cv2\n",
    "import win32gui\n",
    "import csv\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import difflib\n",
    "from datetime import datetime\n",
    "from PIL import ImageGrab, ImageEnhance\n",
    "from paddleocr import PaddleOCR\n",
    "\n",
    "ocr = PaddleOCR(\n",
    "    use_doc_orientation_classify=False,\n",
    "    use_doc_unwarping=False,\n",
    "    use_textline_orientation=False,\n",
    "    text_det_box_thresh=0.6,\n",
    "    lang='ch',\n",
    "    # ä½¿ç”¨è½»é‡æ¨¡å‹\n",
    "    text_detection_model_name='PP-OCRv4_mobile_det',  # æ£€æµ‹æ¨¡å‹\n",
    "    text_recognition_model_name='PP-OCRv4_mobile_rec',  # è¯†åˆ«æ¨¡å‹\n",
    "    text_detection_model_dir='C:\\\\Users\\\\16528\\\\.paddlex\\\\official_models\\\\PP-OCRv4_mobile_det',  # æ£€æµ‹æ¨¡å‹\n",
    "    text_recognition_model_dir='C:\\\\Users\\\\16528\\\\.paddlex\\\\official_models\\\\PP-OCRv4_mobile_rec',  # è¯†åˆ«æ¨¡å‹\n",
    ")\n",
    "\n",
    "custom_dict_path = \"C:\\\\Users\\\\16528\\\\Desktop\\\\custom_words.txt\"  # è‡ªå®šä¹‰è¯åº“è·¯å¾„\n",
    "# ===== é…ç½® =====\n",
    "TARGET_TITLE = \"çŸ­çº¿ç²¾çµ\"  # ç›®æ ‡çª—å£æ ‡é¢˜\n",
    "SAVE_FILE = \"C:\\\\Users\\\\16528\\\\PycharmProjects\\\\PythonProject\\\\save_file\"\n",
    "\n",
    "INTERVAL = 5  # ç§’\n",
    "DEBUG_DIR = \"debug_images\"\n",
    "GRID_ROWS = 1  # è¡Œæ•° (1è¡Œ)\n",
    "GRID_COLS = 5  # åˆ—æ•° (8åˆ—)\n",
    "GRID_PADDING = 0  # åŒºåŸŸé—´çš„åƒç´ å¡«å……\n",
    "CROP_TOP = 60  # é«˜åº¦è£å‰ªé¡¶éƒ¨ä½ç½®\n",
    "CROP_BOTTOM = 1008  # é«˜åº¦è£å‰ªåº•éƒ¨ä½ç½®\n",
    "CROP_LEFT = 0  # å®½åº¦è£å‰ªå·¦ä¾§ä½ç½®\n",
    "CROP_RIGHT = 1920  # å®½åº¦è£å‰ªå³ä¾§ä½ç½®\n",
    "# æ•°æ®åº“è¿æ¥å‚æ•°\n",
    "host = \"localhost\"\n",
    "database = \"public\"\n",
    "user = \"root\"\n",
    "password = \"root\"\n",
    "\n",
    "# è·å–ç³»ç»Ÿæ—¥æœŸ\n",
    "date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# æ–°å»ºå­ç›®å½•\n",
    "save_path = os.path.join(SAVE_FILE, date_str)\n",
    "folder_path = os.path.join(SAVE_FILE, date_str)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# æ‹¼æ¥ä¿å­˜æ–‡ä»¶è·¯å¾„\n",
    "TXT_FILE = os.path.join(save_path, \"å¤§ç¬”ä¹°å…¥.txt\")\n",
    "CSV_FILE = os.path.join(save_path, \"å¤§ç¬”ä¹°å…¥.csv\")\n",
    "# è¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°\n",
    "output_path = os.path.join(folder_path, \"ç»Ÿè®¡ç»“æœ.txt\")\n",
    "last_file_path=os.path.join(folder_path, \"last_big_amount.txt\")\n",
    "\n",
    "def find_window_rect(title):\n",
    "    hwnd = win32gui.FindWindow(None, title)\n",
    "    if hwnd == 0:\n",
    "        print(f\"âŒ æ‰¾ä¸åˆ°çª—å£: {title}\")\n",
    "        return None\n",
    "    return win32gui.GetWindowRect(hwnd)\n",
    "\n",
    "# ====== è¯»å–è‡ªå®šä¹‰è¯åº“ ======\n",
    "with open(custom_dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    custom_words = [w.strip() for w in f.read().splitlines() if w.strip()]\n",
    "\n",
    "def import_csv_to_mysql(all_rows_csv, host, database, user, password):\n",
    "    try:\n",
    "        # è¿æ¥åˆ°MySQLæ•°æ®åº“\n",
    "        connection = mysql.connector.connect(\n",
    "            host=host,\n",
    "            database=database,\n",
    "            user=user,\n",
    "            password=password\n",
    "        )\n",
    "\n",
    "        if connection.is_connected():\n",
    "            cursor = connection.cursor()\n",
    "            # å‡†å¤‡æ’å…¥è¯­å¥\n",
    "            insert_query = \"\"\"INSERT INTO short_term_elf (\n",
    "                time, stock_name, description, amount,trade_date\n",
    "            ) VALUES (%s, %s, %s, %s, CURDATE() )\"\"\"\n",
    "\n",
    "            # æ’å…¥æ•°æ®\n",
    "            for row in all_rows_csv:\n",
    "                # å¤„ç†ç©ºå€¼\n",
    "                processed_row = []\n",
    "                for value in row:\n",
    "                    if value == '' or value is None:\n",
    "                        processed_row.append(None)\n",
    "                    else:\n",
    "                        processed_row.append(value)\n",
    "\n",
    "                cursor.execute(insert_query, processed_row)\n",
    "\n",
    "            connection.commit()\n",
    "            print(f\"æˆåŠŸå¯¼å…¥ {cursor.rowcount} æ¡è®°å½•åˆ°æ•°æ®åº“\")\n",
    "\n",
    "    except Error as e:\n",
    "        print(f\"æ•°æ®åº“é”™è¯¯: {e}\")\n",
    "    finally:\n",
    "        if connection.is_connected():\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"MySQLè¿æ¥å·²å…³é—­\")\n",
    "\n",
    "\n",
    "def preprocess_image(img):\n",
    "    \"\"\"ä¼˜åŒ–å›¾åƒé¢„å¤„ç†ï¼Œç‰¹åˆ«é’ˆå¯¹å°å­—ä½“\"\"\"\n",
    "    # è½¬æ¢ä¸ºç°åº¦\n",
    "    img = img.convert(\"L\")\n",
    "\n",
    "    # å¢å¼ºå¯¹æ¯”åº¦\n",
    "    enhancer = ImageEnhance.Contrast(img)\n",
    "    img = enhancer.enhance(1.7)\n",
    "\n",
    "    # é”åŒ–å›¾åƒ\n",
    "    enhancer = ImageEnhance.Sharpness(img)\n",
    "    img = enhancer.enhance(1.3)\n",
    "\n",
    "    return img\n",
    "\n",
    "def load_existing_data(file_path):\n",
    "    \"\"\"è¯»å–å·²ä¿å­˜çš„è¡Œï¼Œå»æ‰ç©ºè¡Œ\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return set()\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "        return set(line.strip() for line in f if line.strip())\n",
    "\n",
    "def save_unique_txt(rows):\n",
    "    \"\"\"è¿½åŠ ä¿å­˜åˆ° TXTï¼ˆè‡ªåŠ¨å»é‡ï¼‰\"\"\"\n",
    "    existing = load_existing_data(TXT_FILE)\n",
    "    new_rows = [r for r in rows if r not in existing]\n",
    "    if not new_rows:\n",
    "        return 0\n",
    "    with open(TXT_FILE, 'a', encoding='utf-8') as f:\n",
    "        for row in new_rows:\n",
    "            f.write(row + \"\\n\")\n",
    "    return len(new_rows)\n",
    "\n",
    "def save_unique_csv(rows):\n",
    "    \"\"\"è¿½åŠ ä¿å­˜åˆ° CSVï¼ˆè‡ªåŠ¨å»é‡ï¼‰\"\"\"\n",
    "    existing = load_existing_data(CSV_FILE)\n",
    "    is_new_file = not os.path.exists(CSV_FILE)\n",
    "    with open(CSV_FILE, 'a', newline='', encoding='utf-8-sig') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if is_new_file:\n",
    "            writer.writerow([\"æ—¶é—´\", \"è‚¡ç¥¨\", \"æè¿°\", \"é‡‘é¢\"])\n",
    "        added_count = 0\n",
    "        unique_rows = []  # ç”¨äºè¿”å›æœ€ç»ˆå»é‡åçš„æ•°æ®\n",
    "        for row in rows:\n",
    "            row_str = \",\".join(row)\n",
    "            if row_str not in existing:\n",
    "                writer.writerow(row)\n",
    "                unique_rows.append(row)\n",
    "                added_count += 1\n",
    "        if added_count > 0:\n",
    "            import_csv_to_mysql(unique_rows, host, database, user, password)\n",
    "    return added_count\n",
    "\n",
    "def sort_rows_by_time(txt_rows, csv_rows):\n",
    "    \"\"\"\n",
    "    æŒ‰æ—¶é—´æ’åºæ•°æ®è¡Œ\n",
    "    :param txt_rows: [\"09:31:25| è‚¡ç¥¨ | æè¿° | é‡‘é¢\", ...]\n",
    "    :param csv_rows: [[\"09:31:25\", \"è‚¡ç¥¨\", \"æè¿°\", \"é‡‘é¢\"], ...]\n",
    "    \"\"\"\n",
    "    # æ’åº TXT\n",
    "    txt_rows_sorted = sorted(\n",
    "        txt_rows,\n",
    "        key=lambda r: datetime.strptime(r.split(\"|\")[0].strip(), \"%H:%M:%S\"),\n",
    "    )\n",
    "\n",
    "    # æ’åº CSV\n",
    "    csv_rows_sorted = sorted(\n",
    "        csv_rows,\n",
    "        key=lambda r: datetime.strptime(r[0].strip(), \"%H:%M:%S\")\n",
    "    )\n",
    "\n",
    "    return txt_rows_sorted, csv_rows_sorted\n",
    "\n",
    "import akshare as ak\n",
    "\n",
    "def get_market_cap_map(df):\n",
    "    # åªå–åç§°å’Œæµé€šå¸‚å€¼ä¸¤åˆ—\n",
    "    df_selected = df[[\"åç§°\", \"æµé€šå¸‚å€¼\"]].copy()\n",
    "\n",
    "    # è½¬æ¢ä¸ºä¸‡å…ƒ\n",
    "    df_selected[\"æµé€šå¸‚å€¼\"] = df_selected[\"æµé€šå¸‚å€¼\"] / 10000\n",
    "\n",
    "    # ç”Ÿæˆæ˜ å°„å­—å…¸ {è‚¡ç¥¨åç§°: æµé€šå¸‚å€¼(ä¸‡å…ƒ)}\n",
    "    market_cap_map = dict(zip(df_selected[\"åç§°\"], df_selected[\"æµé€šå¸‚å€¼\"]))\n",
    "\n",
    "    return market_cap_map\n",
    "\n",
    "\n",
    "\n",
    "def match_ocr_lines_to_dict(ocr_lines: list, custom_words: list, cutoff=0.6):\n",
    "    \"\"\"\n",
    "    æ‰¹é‡æ¨¡ç³ŠåŒ¹é… OCR æ–‡æœ¬è¡Œåˆ°è¯åº“\n",
    "\n",
    "    :param ocr_lines: OCR è¯†åˆ«ç»“æœåˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªå…ƒç´ ï¼‰\n",
    "    :param custom_words: è‡ªå®šä¹‰è¯åº“ï¼ˆlistï¼‰\n",
    "    :param cutoff: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0~1ï¼‰ï¼Œè¶Šé«˜åŒ¹é…è¶Šä¸¥æ ¼\n",
    "    :return: åŒ¹é…ç»“æœåˆ—è¡¨ï¼ˆæ¯ä¸ªå…ƒç´ æ˜¯åŒ¹é…åçš„å­—ç¬¦ä¸²ï¼‰\n",
    "    \"\"\"\n",
    "    # ç¡®ä¿è¾“å…¥æ˜¯åˆ—è¡¨ä¸”éç©º\n",
    "    if not isinstance(ocr_lines, list):\n",
    "        raise ValueError(\"ocr_lines å¿…é¡»æ˜¯åˆ—è¡¨ç±»å‹\")\n",
    "\n",
    "    matched_results = []\n",
    "    for line in ocr_lines:\n",
    "        # è·å–æœ€ç›¸ä¼¼çš„åŒ¹é…é¡¹\n",
    "        matches = difflib.get_close_matches(line, custom_words, n=1, cutoff=cutoff)\n",
    "\n",
    "        if matches:\n",
    "            # ä½¿ç”¨åŒ¹é…åˆ°çš„è¯åº“é¡¹\n",
    "            matched_results.append(matches[0])\n",
    "        else:\n",
    "            # æ²¡æœ‰åŒ¹é…åˆ°åˆ™ä¿ç•™åŸæ–‡æœ¬\n",
    "            matched_results.append(line)\n",
    "\n",
    "    return matched_results\n",
    "\n",
    "def split_image_into_regions_reverse(cropped):\n",
    "    width = cropped.shape[1]\n",
    "    col_width = width // GRID_COLS\n",
    "    found_data = False\n",
    "    all_rows_txt = []\n",
    "    all_rows_csv = []\n",
    "\n",
    "    # é€†åºå¾ªç¯\n",
    "    for i in reversed(range(GRID_COLS)):\n",
    "        start_col = i * col_width\n",
    "        end_col = width if i == GRID_COLS - 1 else (i + 1) * col_width\n",
    "        region = cropped[:, start_col:end_col]\n",
    "\n",
    "        # åˆ†å››åˆ—\n",
    "        h, w = region.shape[:2]\n",
    "        col1 = region[:, 0:int(w * 0.25)]\n",
    "        col2 = region[:, int(w * 0.25):int(w * 0.50)]\n",
    "        col3 = region[:, int(w * 0.50):int(w * 0.75)]\n",
    "        col4 = region[:, int(w * 0.75):]\n",
    "\n",
    "        # ä¿å­˜è°ƒè¯•å›¾\n",
    "        cv2.imwrite(\"col1_time.png\", col1)\n",
    "        cv2.imwrite(\"col2_stock.png\", col2)\n",
    "        cv2.imwrite(\"col3_desc.png\", col3)\n",
    "        cv2.imwrite(\"col4_money.png\", col4)\n",
    "\n",
    "        image_path_time = r\"C:\\Users\\16528\\PycharmProjects\\PythonProject\\shortline_ths\\col1_time.png\"\n",
    "        image_path_stock = r\"C:\\Users\\16528\\PycharmProjects\\PythonProject\\shortline_ths\\col2_stock.png\"\n",
    "        image_path_desc = r\"C:\\Users\\16528\\PycharmProjects\\PythonProject\\shortline_ths\\col3_desc.png\"\n",
    "        image_path_money = r\"C:\\Users\\16528\\PycharmProjects\\PythonProject\\shortline_ths\\col4_money.png\"\n",
    "\n",
    "        # OCR\n",
    "        time_text = ocr.predict(image_path_time)\n",
    "        stock_text =ocr.predict(image_path_stock)\n",
    "        desc_text = ocr.predict(image_path_desc)\n",
    "        money_text = ocr.predict(image_path_money)\n",
    "\n",
    "        # æŒ‰è¡Œåˆå¹¶\n",
    "        time_lines = time_text[0]['rec_texts']\n",
    "        stock_lines = stock_text[0]['rec_texts']\n",
    "        stock_lines = match_ocr_lines_to_dict(stock_lines, custom_words, 0.7)\n",
    "        desc_lines = desc_text[0]['rec_texts']\n",
    "        money_lines = money_text[0]['rec_texts']\n",
    "\n",
    "        if not (len(time_lines) == len(stock_lines) == len(money_lines)):\n",
    "            print(f\"é”™è¯¯ï¼šç¬¬ {i} åŒºåŸŸçš„å››åˆ—çš„è¡Œæ•°ä¸ä¸€è‡´ï¼\")\n",
    "            return False\n",
    "\n",
    "        rows = min(len(time_lines), len(stock_lines), len(money_lines))\n",
    "        if rows == 0  or (len(time_lines) == 1 and time_lines[0].strip() == \"\"):\n",
    "            continue  # æ²¡æ•°æ® â†’ å¾€å‰æ‰«æ\n",
    "        found_data = True\n",
    "\n",
    "        # ç»„åˆè¡Œ\n",
    "        for j in range(rows):\n",
    "            # åˆ¤æ–­æ˜¯å¦ä¸ºå–å‡º\n",
    "            money_val = money_lines[j]\n",
    "            if \"å–å‡º\" in desc_lines[j]:\n",
    "                # æ•°å­—å‰åŠ è´Ÿå·\n",
    "                if not money_val.startswith(\"-\"):\n",
    "                    money_val = \"-\" + money_val\n",
    "\n",
    "            txt_line = f\"{time_lines[j].replace(' ', '')} | {stock_lines[j]} | {desc_lines[j].replace('äºº', 'å…¥')} | {money_val}\"\n",
    "            csv_line = [time_lines[j].replace(\" \", \"\"), stock_lines[j], desc_lines[j].replace(\"äºº\", \"å…¥\"), money_val]\n",
    "            all_rows_txt.append(txt_line)\n",
    "            all_rows_csv.append(csv_line)\n",
    "\n",
    "        # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤\n",
    "        existing_txt = load_existing_data(TXT_FILE)\n",
    "        if any(r in existing_txt for r in all_rows_txt):\n",
    "            print(f\"âš ï¸ ç¬¬ {i} åŒºåŸŸæ£€æµ‹åˆ°é‡å¤æ•°æ® â†’ åœæ­¢æœ¬è½®æ‰«æ\")\n",
    "            break  # åœæ­¢æœ¬æ¬¡æ‰«æ\n",
    "        print(f\"æœ¬è½®æ‰«æç¬¬ {i} åŒºåŸŸæ£€æµ‹å·²å®Œæˆ\")\n",
    "\n",
    "    if not found_data:\n",
    "        print(\"âŒ æœ¬è½®æ‰«ææœªè¯†åˆ«åˆ°ä»»ä½•æ•°æ®\")\n",
    "    else:\n",
    "        # æŒ‰æ—¶é—´æ’åº\n",
    "        all_rows_txt, all_rows_csv = sort_rows_by_time(all_rows_txt, all_rows_csv)\n",
    "        added_txt = save_unique_txt(all_rows_txt)\n",
    "        added_csv = save_unique_csv(all_rows_csv)\n",
    "        print(f\"ğŸ’¾ æœ¬è½®æ–°å¢ TXT {added_txt} è¡Œ, CSV {added_csv} è¡Œï¼ˆå·²æŒ‰æ—¶é—´æ’åºï¼‰\")\n",
    "\n",
    "def output_big_amount_from_file(market_cap_map,file_path, threshold=3000, save_path=\"last_big_amount.txt\"):\n",
    "\n",
    "    \"\"\"\n",
    "    è¾“å‡ºæˆäº¤é‡‘é¢å¤§äº threshold ä¸‡å…ƒçš„è‚¡ç¥¨ï¼ŒåŒæ—¶å’Œä¸Šä¸€æ¬¡ç»“æœåšå¯¹æ¯”ï¼ˆâ†‘ â†“ï¼‰\n",
    "    \"\"\"\n",
    "    # 1. è¯»å–ä¸Šæ¬¡ç»“æœ\n",
    "    last_result = {}\n",
    "    if os.path.exists(save_path):\n",
    "        with open(save_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(\",\")\n",
    "                if len(parts) == 3:\n",
    "                    name, amount, pct = parts\n",
    "                    try:\n",
    "                        last_result[name] = float(amount)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "\n",
    "    big_amount_stocks = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            # æå–æˆäº¤æ€»é‡‘é¢æ•°å­—\n",
    "            match_amount = re.search(r\"æˆäº¤æ€»é‡‘é¢ä¸º([\\d.]+)ä¸‡å…ƒ\", line)\n",
    "            if match_amount:\n",
    "                total_amount = float(match_amount.group(1))\n",
    "                if total_amount > threshold:\n",
    "                    # æå–ç¬¬ä¸€ä¸ªéç©ºçš„è‚¡ç¥¨åç§°ï¼ˆå‡è®¾è‚¡ç¥¨ååœ¨å¼€å¤´ï¼Œä¸­æ–‡æˆ–æ•°å­—ä»£ç éƒ½å¯ï¼‰\n",
    "                    match_name = re.match(r\"([^\\sï¼Œ,]+)\", line.strip())\n",
    "                    if match_name:\n",
    "                        stock_name = match_name.group(1)\n",
    "                        pct = round(total_amount/market_cap_map[stock_name] * 100,2)\n",
    "                        if pct > 0.3:\n",
    "                            big_amount_stocks.append((stock_name, total_amount, pct))\n",
    "\n",
    "    # æŒ‰é‡‘é¢é™åº\n",
    "    big_amount_stocks.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # 3. è¾“å‡º\n",
    "    if big_amount_stocks:\n",
    "        print(f\"\\nğŸ’° æˆäº¤æ€»é‡‘é¢ > {threshold} ä¸‡å…ƒçš„è‚¡ç¥¨ï¼š\")\n",
    "        formatted = []\n",
    "        printStockName =[]\n",
    "        for name, amount, pct in big_amount_stocks:\n",
    "            if name in last_result:\n",
    "                cz =amount-last_result[name]\n",
    "                if amount > last_result[name]:\n",
    "                    amount = f\"{amount}ğŸ”ºğŸ”ºğŸ”º{cz}\"\n",
    "                elif amount < last_result[name]:\n",
    "                    amount = f\"{amount}ğŸ”½ğŸ”½ğŸ”½{cz}\"\n",
    "                elif amount == last_result[name]:\n",
    "                    amount = f\"{amount}â–\"\n",
    "            else:\n",
    "                amount = f\"{amount}ğŸ”º\"   # é‡‘é¢æŸ“çº¢\n",
    "            if pct > 0.7:\n",
    "                pct = f\"{pct}%â¤ï¸\"\n",
    "            formatted.append(f\"{name}ï¼š{amount}: {pct}\")\n",
    "            printStockName.append(f\"{name}\")\n",
    "        print(\"  |\" .join(formatted))\n",
    "        print(f\"\\n æ»¡è¶³æ¡ä»¶çš„è‚¡ç¥¨åç§°{printStockName}\")\n",
    "    else:\n",
    "        print(f\"\\næ²¡æœ‰æˆäº¤æ€»é‡‘é¢å¤§äº {threshold} ä¸‡å…ƒçš„è‚¡ç¥¨\")\n",
    "\n",
    "    # 4. ä¿å­˜æœ¬æ¬¡ç»“æœï¼Œè¦†ç›–æ–‡ä»¶\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for name, amount, pct in big_amount_stocks:\n",
    "            f.write(f\"{name},{amount},{pct}\\n\")\n",
    "\n",
    "    sys.stdout.flush()  # åŒé‡ç¡®ä¿åˆ·æ–°\n",
    "\n",
    "def tj():\n",
    "\n",
    "    # åŒ¹é…æ¨¡å¼ï¼šç¬¬ä¸€åˆ—æ•°å­—æˆ–å†’å·ç»„åˆï¼Œç¬¬äºŒåˆ—è‚¡ç¥¨åï¼Œç¬¬å››åˆ—é‡‘é¢\n",
    "    # æ”¹è¿›åçš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œæ”¯æŒè´Ÿå·é‡‘é¢åŒ¹é…\n",
    "    pattern = re.compile(\n",
    "        r\"^\\s*([\\d:]+)\\s*\\|\\s*([\\w\\u4e00-\\u9fa5\\s-]+)\\s*\\|\\s*([\\u4e00-\\u9fa5\\s]+)\\s*\\|\\s*(-?[\\d\\.]+)ä¸‡\"\n",
    "    )\n",
    "\n",
    "    stock_data = {}\n",
    "\n",
    "    # éå†ç›®å½•ä¸‹æ‰€æœ‰ txt æ–‡ä»¶\n",
    "    txt_files = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "\n",
    "    for file_path in txt_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if \"|\" not in line:\n",
    "                    continue\n",
    "                match = pattern.match(line)\n",
    "                if match:\n",
    "                    time_str, stock_name, note, amount_str = match.groups()\n",
    "                    # ä¿ç•™æ­£è´Ÿå·\n",
    "                    amount = int(amount_str) if not amount_str.startswith('-') else int(amount_str)\n",
    "\n",
    "                    if stock_name not in stock_data:\n",
    "                        stock_data[stock_name] = {\n",
    "                            \"count\": 0,\n",
    "                            \"total\": 0,\n",
    "                            \"early_count\": 0, \"early_amount\": 0,\n",
    "                            \"mid_count\": 0, \"mid_amount\": 0,\n",
    "                            \"late_count\": 0, \"late_amount\": 0,\n",
    "                            \"details\": []\n",
    "                        }\n",
    "\n",
    "                    # ç›´æ¥æŒ‰æ­£è´Ÿç´¯åŠ  â†’ ä¹°å…¥åŠ ã€å–å‡ºå‡\n",
    "                    stock_data[stock_name][\"count\"] += 1\n",
    "                    stock_data[stock_name][\"total\"] += amount\n",
    "                    stock_data[stock_name][\"details\"].append((time_str, amount))\n",
    "\n",
    "                    # æ—¶é—´æ®µåˆ†ç±»\n",
    "                    try:\n",
    "                        hour, minute, _ = map(int, time_str.split(\":\"))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                    time_val = hour * 60 + minute\n",
    "                    if time_val < 9 * 60 + 45:\n",
    "                        stock_data[stock_name][\"early_count\"] += 1\n",
    "                        stock_data[stock_name][\"early_amount\"] += amount\n",
    "                    elif time_val <= 14 * 60 + 45:\n",
    "                        stock_data[stock_name][\"mid_count\"] += 1\n",
    "                        stock_data[stock_name][\"mid_amount\"] += amount\n",
    "                    else:\n",
    "                        stock_data[stock_name][\"late_count\"] += 1\n",
    "                        stock_data[stock_name][\"late_amount\"] += amount\n",
    "\n",
    "\n",
    "    # æŒ‰å‡ºç°æ¬¡æ•°é™åº\n",
    "    sorted_data = sorted(stock_data.items(), key=lambda x: x[1][\"total\"], reverse=True)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as out_file:\n",
    "        for stock, info in sorted_data:\n",
    "            # ä¹°å…¥ / å–å‡ºæ€»è®¡\n",
    "            buy_count = sum(1 for _, a in info[\"details\"] if a > 0)\n",
    "            buy_amount = sum(a for _, a in info[\"details\"] if a > 0)\n",
    "            sell_count = sum(1 for _, a in info[\"details\"] if a < 0)\n",
    "            sell_amount = sum(abs(a) for _, a in info[\"details\"] if a < 0)\n",
    "\n",
    "            # æˆäº¤æ€»é‡‘é¢ = ä¹°å…¥é‡‘é¢ - å–å‡ºé‡‘é¢\n",
    "            total_trades = info[\"count\"]\n",
    "            total_amount = buy_amount - sell_amount  # â† è¿™é‡Œæ”¹æˆå·®å€¼\n",
    "\n",
    "            # æ—©ç›˜ã€ä¸­ç›˜ã€å°¾ç›˜çš„ä¹°å…¥/å–å‡ºç»†åˆ†\n",
    "            def period_stats(start_min, end_min):\n",
    "                trades = [(t, a) for t, a in info[\"details\"]\n",
    "                          if start_min <= (int(t.split(\":\")[0]) * 60 + int(t.split(\":\")[1])) <= end_min]\n",
    "                period_count = len(trades)\n",
    "                # å‡€é¢ = ä¹°å…¥ - å–å‡º\n",
    "                period_amount = sum(a for _, a in trades)\n",
    "                buy_c = sum(1 for _, a in trades if a > 0)\n",
    "                buy_a = sum(a for _, a in trades if a > 0)\n",
    "                sell_c = sum(1 for _, a in trades if a < 0)\n",
    "                sell_a = sum(abs(a) for _, a in trades if a < 0)\n",
    "                return period_count, period_amount, buy_c, buy_a, sell_c, sell_a\n",
    "\n",
    "            early_stats = period_stats(0, 9 * 60 + 44)\n",
    "            mid_stats   = period_stats(9 * 60 + 45, 14 * 60 + 45)\n",
    "            late_stats  = period_stats(14 * 60 + 46, 24 * 60)\n",
    "\n",
    "            details_str = \"ï¼Œ\".join([f\"{t}{'ä¹°å…¥' if a>0 else 'å–å‡º'}{abs(a)}ä¸‡å…ƒ\" for t, a in info[\"details\"]])\n",
    "\n",
    "            out_file.write(\n",
    "                f\"{stock} ï¼Œæˆäº¤{total_trades}æ¬¡ï¼Œæˆäº¤æ€»é‡‘é¢ä¸º{total_amount}ä¸‡å…ƒï¼›\"\n",
    "                f\"ä¹°å…¥{buy_count}æ¬¡é‡‘é¢{buy_amount}ä¸‡å…ƒï¼Œå–å‡º{sell_count}æ¬¡é‡‘é¢{sell_amount}ä¸‡å…ƒ\\n\"\n",
    "                f\"    æ—©ç›˜ï¼šæˆäº¤{early_stats[0]}æ¬¡ï¼Œæˆäº¤é‡‘é¢{early_stats[1]}ä¸‡å…ƒï¼›ä¹°å…¥{early_stats[2]}æ¬¡é‡‘é¢{early_stats[3]}ä¸‡å…ƒï¼›å–å‡º{early_stats[4]}æ¬¡é‡‘é¢{early_stats[5]}ä¸‡å…ƒ\\n\"\n",
    "                f\"    ä¸­ç›˜ï¼šæˆäº¤{mid_stats[0]}æ¬¡ï¼Œæˆäº¤é‡‘é¢{mid_stats[1]}ä¸‡å…ƒï¼›ä¹°å…¥{mid_stats[2]}æ¬¡é‡‘é¢{mid_stats[3]}ä¸‡å…ƒï¼›å–å‡º{mid_stats[4]}æ¬¡é‡‘é¢{mid_stats[5]}ä¸‡å…ƒ\\n\"\n",
    "                f\"    å°¾ç›˜ï¼šæˆäº¤{late_stats[0]}æ¬¡ï¼Œæˆäº¤é‡‘é¢{late_stats[1]}ä¸‡å…ƒï¼›ä¹°å…¥{late_stats[2]}æ¬¡é‡‘é¢{late_stats[3]}ä¸‡å…ƒï¼›å–å‡º{late_stats[4]}æ¬¡é‡‘é¢{late_stats[5]}ä¸‡å…ƒ\\n\"\n",
    "                f\"    è¯¦æƒ…ï¼šåˆ†åˆ«åœ¨{details_str}\\n\\n\"\n",
    "            )\n",
    "\n",
    "    print(f\"\\nç»Ÿè®¡å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {output_path}\")\n",
    "\n",
    "def parse_text_to_csv(input_file_path, output_file_path=None):\n",
    "    # å¦‚æœæœªæŒ‡å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œåˆ™ä½¿ç”¨è¾“å…¥æ–‡ä»¶åŒåä½†æ‰©å±•åä¸º.csv\n",
    "    if output_file_path is None:\n",
    "        output_file_path = os.path.splitext(input_file_path)[0] + '.csv'\n",
    "\n",
    "    # è¯»å–è¾“å…¥æ–‡ä»¶å†…å®¹\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # åˆ†å‰²å†…å®¹ä¸ºå¤šä¸ªè®°å½•ï¼ˆå‡è®¾æ¯ä¸ªè®°å½•ç”±ç©ºè¡Œåˆ†éš”ï¼‰\n",
    "    records = content.strip().split('\\n\\n')\n",
    "\n",
    "    # å®šä¹‰CSVè¡¨å¤´\n",
    "    headers = [\n",
    "        'è‚¡ç¥¨åç§°', 'æ€»æˆäº¤æ¬¡æ•°', 'æ€»æˆäº¤é‡‘é¢(ä¸‡å…ƒ)', 'æ€»ä¹°å…¥æ¬¡æ•°', 'æ€»ä¹°å…¥é‡‘é¢(ä¸‡å…ƒ)',\n",
    "        'æ€»å–å‡ºæ¬¡æ•°', 'æ€»å–å‡ºé‡‘é¢(ä¸‡å…ƒ)',\n",
    "        'æ—©ç›˜æˆäº¤æ¬¡æ•°', 'æ—©ç›˜æˆäº¤é‡‘é¢(ä¸‡å…ƒ)', 'æ—©ç›˜ä¹°å…¥æ¬¡æ•°', 'æ—©ç›˜ä¹°å…¥é‡‘é¢(ä¸‡å…ƒ)',\n",
    "        'æ—©ç›˜å–å‡ºæ¬¡æ•°', 'æ—©ç›˜å–å‡ºé‡‘é¢(ä¸‡å…ƒ)',\n",
    "        'ä¸­ç›˜æˆäº¤æ¬¡æ•°', 'ä¸­ç›˜æˆäº¤é‡‘é¢(ä¸‡å…ƒ)', 'ä¸­ç›˜ä¹°å…¥æ¬¡æ•°', 'ä¸­ç›˜ä¹°å…¥é‡‘é¢(ä¸‡å…ƒ)',\n",
    "        'ä¸­ç›˜å–å‡ºæ¬¡æ•°', 'ä¸­ç›˜å–å‡ºé‡‘é¢(ä¸‡å…ƒ)',\n",
    "        'å°¾ç›˜æˆäº¤æ¬¡æ•°', 'å°¾ç›˜æˆäº¤é‡‘é¢(ä¸‡å…ƒ)', 'å°¾ç›˜ä¹°å…¥æ¬¡æ•°', 'å°¾ç›˜ä¹°å…¥é‡‘é¢(ä¸‡å…ƒ)',\n",
    "        'å°¾ç›˜å–å‡ºæ¬¡æ•°', 'å°¾ç›˜å–å‡ºé‡‘é¢(ä¸‡å…ƒ)',\n",
    "        'è¯¦æƒ…'\n",
    "    ]\n",
    "\n",
    "    # å‡†å¤‡æ‰€æœ‰è®°å½•çš„æ•°æ®\n",
    "    all_data = []\n",
    "\n",
    "    for record in records:\n",
    "        # è·³è¿‡ç©ºè®°å½•\n",
    "        if not record.strip():\n",
    "            continue\n",
    "\n",
    "        # æå–è‚¡ç¥¨åç§°\n",
    "        stock_name_match = re.match(r'(\\S+)\\s*ï¼Œ', record)\n",
    "        if not stock_name_match:\n",
    "            continue\n",
    "\n",
    "        stock_name = stock_name_match.group(1)\n",
    "        data = [stock_name]\n",
    "\n",
    "        # æå–æ€»ä½“æ•°æ®\n",
    "        # æ€»æˆäº¤æ¬¡æ•°å’Œé‡‘é¢\n",
    "        total_trades_match = re.search(r'æˆäº¤(\\d+)æ¬¡', record)\n",
    "        total_amount_match = re.search(r'æˆäº¤æ€»é‡‘é¢ä¸º(-?\\d+)ä¸‡å…ƒ', record)\n",
    "\n",
    "        # ä¹°å…¥æ¬¡æ•°å’Œé‡‘é¢ï¼ˆæ³¨æ„ï¼šå¯èƒ½ä¸ºè´Ÿæ•°ï¼‰\n",
    "        buy_times_match = re.search(r'ä¹°å…¥(\\d+)æ¬¡', record)\n",
    "        buy_amount_match = re.search(r'ä¹°å…¥.*?é‡‘é¢(-?\\d+)ä¸‡å…ƒ', record)\n",
    "\n",
    "        # å–å‡ºæ¬¡æ•°å’Œé‡‘é¢ï¼ˆæ³¨æ„ï¼šå¯èƒ½ä¸ºè´Ÿæ•°ï¼‰\n",
    "        sell_times_match = re.search(r'å–å‡º(\\d+)æ¬¡', record)\n",
    "        sell_amount_match = re.search(r'å–å‡º.*?é‡‘é¢(-?\\d+)ä¸‡å…ƒ', record)\n",
    "\n",
    "        # æ·»åŠ åˆ°æ•°æ®åˆ—è¡¨\n",
    "        data.append(total_trades_match.group(1) if total_trades_match else '0')\n",
    "        data.append(total_amount_match.group(1) if total_amount_match else '0')\n",
    "        data.append(buy_times_match.group(1) if buy_times_match else '0')\n",
    "        data.append(buy_amount_match.group(1) if buy_amount_match else '0')\n",
    "        data.append(sell_times_match.group(1) if sell_times_match else '0')\n",
    "        data.append(sell_amount_match.group(1) if sell_amount_match else '0')\n",
    "\n",
    "        # æå–å„æ—¶é—´æ®µæ•°æ®\n",
    "        time_periods = ['æ—©ç›˜', 'ä¸­ç›˜', 'å°¾ç›˜']\n",
    "        for period in time_periods:\n",
    "            # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼æå–æ¯ä¸ªæ—¶é—´æ®µçš„æ•°æ®\n",
    "            period_pattern = f'{period}ï¼š.*?æˆäº¤(\\\\d+)æ¬¡ï¼Œæˆäº¤é‡‘é¢(-?\\\\d+)ä¸‡å…ƒï¼›ä¹°å…¥(\\\\d+)æ¬¡é‡‘é¢(-?\\\\d+)ä¸‡å…ƒï¼›å–å‡º(\\\\d+)æ¬¡é‡‘é¢(-?\\\\d+)ä¸‡å…ƒ'\n",
    "            match = re.search(period_pattern, record)\n",
    "            if match:\n",
    "                data.extend(match.groups())\n",
    "            else:\n",
    "                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…ï¼Œå°è¯•æ›´å®½æ¾çš„åŒ¹é…\n",
    "                loose_pattern = f'{period}ï¼š.*?æˆäº¤(\\\\d+)æ¬¡.*?æˆäº¤é‡‘é¢(-?\\\\d+)ä¸‡å…ƒ.*?ä¹°å…¥(\\\\d+)æ¬¡.*?é‡‘é¢(-?\\\\d+)ä¸‡å…ƒ.*?å–å‡º(\\\\d+)æ¬¡.*?é‡‘é¢(-?\\\\d+)ä¸‡å…ƒ'\n",
    "                loose_match = re.search(loose_pattern, record, re.DOTALL)\n",
    "                if loose_match:\n",
    "                    data.extend(loose_match.groups())\n",
    "                else:\n",
    "                    data.extend(['0'] * 6)\n",
    "\n",
    "        # æå–è¯¦æƒ…\n",
    "        detail_match = re.search(r'è¯¦æƒ…ï¼š(.*?)(?=\\n|$)', record, re.DOTALL)\n",
    "        detail = detail_match.group(1).strip() if detail_match else ''\n",
    "        data.append(detail)\n",
    "\n",
    "        all_data.append(data)\n",
    "\n",
    "    # å†™å…¥CSVæ–‡ä»¶\n",
    "    with open(output_file_path, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(all_data)\n",
    "\n",
    "    print(f\"æˆåŠŸå¤„ç† {len(all_data)} æ¡è®°å½•ï¼Œç»“æœå·²ä¿å­˜åˆ° {output_file_path}\")\n",
    "    return output_file_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # è·å–Aè‚¡å®æ—¶è¡Œæƒ…æ•°æ®\n",
    "    print(\"ğŸ“¡ å¼€å§‹è·å–aè‚¡å®æ—¶è¡Œæƒ…æ•°æ®...\")\n",
    "    df = ak.stock_zh_a_spot_em()\n",
    "    market_cap_map = get_market_cap_map(df)\n",
    "    print(\"ğŸ“¡ å¼€å§‹å®æ—¶ç›‘æ§çª—å£å†…å®¹...\")\n",
    "    try:\n",
    "        while True:\n",
    "            # æŸ¥æ‰¾ç›®æ ‡çª—å£\n",
    "            rect = find_window_rect(TARGET_TITLE)\n",
    "\n",
    "            if rect:\n",
    "                print(f\"ğŸ” çª—å£ä½ç½®: {rect}\")\n",
    "                # æˆªå–ç‰¹å®šåŒºåŸŸ (å·¦ä¸Šè§’100,100 åˆ°å³ä¸‹è§’500,500)\n",
    "                rect = (0, 0, 1920, 1080)\n",
    "                # æ•è·å¹¶å¤„ç†çª—å£å›¾åƒ\n",
    "                img_np = ImageGrab.grab(bbox=rect)\n",
    "                img_np = preprocess_image(img_np)\n",
    "                # ä¿å­˜åŸå§‹å›¾åƒ\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                img_np.save(f\"{DEBUG_DIR}/{timestamp}_raw.png\")\n",
    "                # è¯»å–åŸå›¾\n",
    "                img = cv2.imread(f\"{DEBUG_DIR}/{timestamp}_raw.png\")\n",
    "                # å…¨å›¾è£å‰ª\n",
    "                cropped = img[CROP_TOP:CROP_BOTTOM, CROP_LEFT:CROP_RIGHT]\n",
    "                split_image_into_regions_reverse(cropped)\n",
    "                tj()\n",
    "                output_big_amount_from_file(market_cap_map,output_path,1500,last_file_path)\n",
    "                parse_text_to_csv(output_path)\n",
    "                print(f\"ä¸‹ä¸€æ¬¡æˆªå±é‡‡é›†å›¾åƒæ—¶ï¼Œåœæ­¢{INTERVAL}ç§’\")\n",
    "                sleep(INTERVAL)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nğŸ›‘ ç¨‹åºå·²æ‰‹åŠ¨åœæ­¢\")\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
